<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[神箭手云爬虫]]></title>
    <url>%2F2019%2F06%2F04%2F%E7%A5%9E%E7%AE%AD%E6%89%8B%E4%BA%91%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[分为： 入口页，帮助页，内容页 入口页： scanUrl 爬虫网页的入口 帮助页：helperurl 一帮包含大量的内容页（列表），多数情况下事业个内容列表，也叫列表页 内容页: contentUrl 承载了我们关心的数据，只在这种网页上做数据抽取，对于需要的部分数据显示在列表上，通过urlContent来解决 待爬队列scanUrl队列： 存放入口类型的Url，configs.scanUrl,定义的链接会放在此队列中，回调可以通过：site.addScanUrl添加链接到此队列中 contentUrl队列：此队列中存放内容页类型的链接，自动链接发现以及site.addUrl的时候，通过正则表达式来判断链接类型，如果是内容页，则将会加到此链接中 普通队列：一般存放的都是帮助页类型的链接，自动链接发现以及site.addUrl 的时候，通过正则表达式来判断链接类型，如果不是内容页，将会加到此队列中 失败队列：一种特殊的队列，用来存放处理失败的链接。 链接调度一次调度一个链接：默认情况下contentUrl队列&gt;普通队列&gt;scanUrl队列 当configs.entriesFirst为true时，调度优先级为：scanUrl队列&gt;contentUrl队列&gt;普通队列 在链接调度过程中，如果遇到了处理失败的情况： 首先会触发链接的重试（此链接会被重新加载到队列中），当重试次数（入口页：5次；帮助页：3次；内容页2次）仍然失败时，此链接就会被放到失败链接，当其他所有队列都为空时，失败队列里面的链接再重试一次，这些链接重试完成后，爬虫结束。 链接去重网站之中链接存在循环，爬虫在爬去的过程中，如果不做控制，很容易陷入死循环。 比较好的方式就是对已经处理过的链接做标记，进行去重处理，（因为大部分处理过的链接不需要再处理一次，另一方面避免爬虫进入死循环） 对于GET类型的请求：平台使用链接本身去重，但是!!!!平台不会对#做处理，www.baidu.com/和 www.baidu.com/#是两个不同的链接 对于POST请求，使用链接+参数的方式去重，也就是说对于同一链接，如果post的参数不一样，平台会认为是两个不同的链接==HTTP的header不会作为去重的依据==]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F04%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
